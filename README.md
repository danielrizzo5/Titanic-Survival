# Titanic Kaggle Competition
This is project aims to predict survival on the titanic based on various passenger information provided by the ["getting started" kaggle competition](https://www.kaggle.com/competitions/titanic/data). Given the historical nature of the data, it is possible to predict exactly the outcome of the test set by searching the historical record. However, in the spirit of the problem, the approach taken here assumed ignorance of the ground truth of the test set.

In the first part of the notebook I organize the numerical features to account for missing entries, outliers, and wide numerical spread. I also encode the non-numerical features (e.g., Sex, Cabin Number), trying first one-hot encoding and then ordinal encoding when appropriate.

I then sample a few classification models using F1 score as the scoring metric due to the imbalanced nature of the output feature (survival). Using the most successful model (XGBoost), I then optimize hyperparameters and check for overfitting by splitting the provided "Training" data into training/dev subsets. Features are then pruned based on minimum feature `importance_type='weight'`, with the subsequent `filtered_features` being analyzed with SHAP analysis. The SHAP summary informs the utility of certain input features, and motivated the replacement of certain one-hot encoded features with ordinal encoded equivalents. SHAP also promptes the introduction of `'data_missing'` features for masked features with a large amount of missing or undefined entries. The best accuracy achieved with this approach was 0.77990, which appears to be comparable with others' who engaged with the problem in good faith (i.e., without consulting the historical record).
